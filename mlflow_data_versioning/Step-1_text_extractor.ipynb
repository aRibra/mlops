{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5g8SefEpeAf",
    "outputId": "1d0ded39-2f64-4a79-c389-7724ccfb58fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install langchain arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TbwH4uHQpyn9"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import  ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow.data\n",
    "from mlflow.data.pandas_dataset import PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6edPqpsFp0bU"
   },
   "outputs": [],
   "source": [
    "arxivv = \"2307.08621\"\n",
    "dataset_source_url = f\"https://arxiv.org/abs/{arxivv}\"\n",
    "\n",
    "docs = ArxivLoader(query=arxivv).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uMyrLZZfYYs",
    "outputId": "a80ba2c9-938e-4dbb-9973-f808aeb50c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mi5IJq1ThUbD",
    "outputId": "8a5148d2-649c-4c90-8ab1-6d8b37142a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_U5pGCmVhsbI",
    "outputId": "9d06ab96-4af0-422a-f9d4-a26fc423c025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kocxFx7mqAlQ",
    "outputId": "667715bf-ee6f-43f0-d843-6b9e326b454a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  Retentive Network: A Successor to Transformer\n",
      "for Large Language Models\n",
      "Yutao Sun∗†‡\n",
      "Li Dong∗†\n",
      "Shaohan Huang†\n",
      "Shuming Ma†\n",
      "Yuqing Xia†\n",
      "Jilong Xue†\n",
      "Jianyong Wang‡\n",
      "Furu Wei†⋄\n",
      "† Microsoft Research\n",
      "‡ Tsinghua University\n",
      "https://aka.ms/GeneralAI\n",
      "Abstract\n",
      "In this work, we propose Retentive Network (RETNET) as a foundation archi-\n",
      "tecture for large language models, simultaneously achieving training parallelism,\n",
      "low-cost inference, and good performance. We theoretically derive the connection\n",
      "between recurrence and attention. Then we propose the retention mechanism for\n",
      "sequence modeling, which supports three computation paradigms, i.e., parallel,\n",
      "recurrent, and chunkwise recurrent. Specifically, the parallel representation allows\n",
      "for training parallelism. The recurrent representation enables low-cost O(1) infer-\n",
      "ence, which improves decoding throughput, latency, and GPU memory without\n",
      "sacrificing performance. The chunkwise recurrent representation facilitates effi-\n",
      "cient long-sequence modeling wi\n"
     ]
    }
   ],
   "source": [
    "print(\"sample: \", docs[0].page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhtbGfldp-f6",
    "outputId": "bbf6a1b8-9fc9-4002-bf26-830827837f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata {'Published': '2023-08-09', 'Title': 'Retentive Network: A Successor to Transformer for Large Language Models', 'Authors': 'Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei', 'Summary': 'In this work, we propose Retentive Network (RetNet) as a foundation\\narchitecture for large language models, simultaneously achieving training\\nparallelism, low-cost inference, and good performance. We theoretically derive\\nthe connection between recurrence and attention. Then we propose the retention\\nmechanism for sequence modeling, which supports three computation paradigms,\\ni.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel\\nrepresentation allows for training parallelism. The recurrent representation\\nenables low-cost $O(1)$ inference, which improves decoding throughput, latency,\\nand GPU memory without sacrificing performance. The chunkwise recurrent\\nrepresentation facilitates efficient long-sequence modeling with linear\\ncomplexity, where each chunk is encoded parallelly while recurrently\\nsummarizing the chunks. Experimental results on language modeling show that\\nRetNet achieves favorable scaling results, parallel training, low-cost\\ndeployment, and efficient inference. The intriguing properties make RetNet a\\nstrong successor to Transformer for large language models. Code will be\\navailable at https://aka.ms/retnet.'}\n"
     ]
    }
   ],
   "source": [
    "print('metadata', docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spilit text data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Fem6SSK7u8Cp"
   },
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_-AhB_DGvQp0"
   },
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMr_BcmTvL6q",
    "outputId": "c9af435d-b70f-456c-ca44-e3c239983954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits:  51\n"
     ]
    }
   ],
   "source": [
    "print(\"splits: \", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDmQejmqRzbV",
    "outputId": "631c8b30-5bd6-4e22-b4d8-586ee37a3455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retentive Network: A Successor to Transformer\n",
      "for Large Language Models\n",
      "Yutao Sun∗†‡\n",
      "Li Dong∗†\n",
      "Shaohan Huang†\n",
      "Shuming Ma†\n",
      "Yuqing Xia†\n",
      "Jilong Xue†\n",
      "Jianyong Wang‡\n",
      "Furu Wei†⋄\n",
      "† Microsoft Research\n",
      "‡ Tsinghua University\n",
      "https://aka.ms/GeneralAI\n",
      "Abstract\n",
      "In this work, we propose Retentive Network (RETNET) as a foundation archi-\n",
      "tecture for large language models, simultaneously achieving training parallelism,\n",
      "low-cost inference, and good performance. We theoretically derive the connection\n",
      "between recurrence and attention. Then we propose the retention mechanism for\n",
      "sequence modeling, which supports three computation paradigms, i.e., parallel,\n",
      "recurrent, and chunkwise recurrent. Specifically, the parallel representation allows\n",
      "for training parallelism. The recurrent representation enables low-cost O(1) infer-\n",
      "ence, which improves decoding throughput, latency, and GPU memory without\n",
      "sacrificing performance. The chunkwise recurrent representation facilitates effi-\n"
     ]
    }
   ],
   "source": [
    "print(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q5H2AHi3S7H5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([split.page_content for split in splits], columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GefSHuVvTED_",
    "outputId": "d4be44b8-d13b-4931-826e-57b510f53aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2UA4pgUQTE6j",
    "outputId": "ac7c8858-5a27-4361-be3e-3eb74e947211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Retentive Network: A Successor to Transformer\\...\n",
      "1  ence, which improves decoding throughput, late...\n",
      "2  Figure 1: Retentive network (RetNet) achieves ...\n",
      "3  els [BMR+20], which was initially proposed\\nto...\n",
      "4  “impossible triangle” as shown in Figure 2.\\nT...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  ['ence, which improves decoding throughput, latency, and GPU memory without\\nsacrificing performance. The chunkwise recurrent representation facilitates effi-\\ncient long-sequence modeling with linear complexity, where each chunk is encoded\\nparallelly while recurrently summarizing the chunks. Experimental results on\\nlanguage modeling show that RETNET achieves favorable scaling results, parallel\\ntraining, low-cost deployment, and efficient inference. The intriguing properties\\nmake RETNET a strong successor to Transformer for large language models. Code\\nwill be available at https://aka.ms/retnet.\\n0\\n20\\n40\\n0\\n150\\n300\\n0\\n150\\n300\\nGPU Memory↓\\n(GB)\\nThroughput↑\\n(wps)\\nLatency↓\\n(ms)\\n3.4X\\n15.6X\\n8.4X\\nInference Cost\\nScaling Curve\\nRetNet\\nTransformer\\n1\\n3\\n7\\nLM Perplexity\\nModel Size (B)\\nFigure 1: Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput,\\nand latency), training parallelism, and favorable scaling curves compared with Transformer. Results']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample: \", df.iloc[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B9lgulVpSPyd"
   },
   "outputs": [],
   "source": [
    "csv_data_path = f\"text_data/text_train_{arxivv}.csv\"\n",
    "df.to_csv(csv_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version dataset with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aribra/.local/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:150: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'text_data/text_train_2307.08621.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/aribra/.local/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:150: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset: PandasDataset = mlflow.data.from_pandas(df, source=csv_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run():\n",
    "    # Log the dataset to the MLflow Run. Specify the \"training\" context to indicate that the\n",
    "    # dataset is used for model training\n",
    "    mlflow.log_input(dataset, context=\"text_training\", tags={f'arxiv': arxivv})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the run, including dataset information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21c21a83b30749df8752300eec2ec4b6'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = mlflow.last_active_run().info.run_id\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: dataset\n",
      "Dataset digest: a42f2dca\n",
      "Dataset profile: {\"num_rows\": 51, \"num_elements\": 51}\n",
      "Dataset schema: {\"mlflow_colspec\": [{\"type\": \"string\", \"name\": \"text\", \"required\": true}]}\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.get_run(mlflow.last_active_run().info.run_id)\n",
    "dataset_info = run.inputs.dataset_inputs[0].dataset\n",
    "print(f\"Dataset name: {dataset_info.name}\")\n",
    "print(f\"Dataset digest: {dataset_info.digest}\")\n",
    "print(f\"Dataset profile: {dataset_info.profile}\")\n",
    "print(f\"Dataset schema: {dataset_info.schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dataset: digest='a42f2dca', name='dataset', profile='{\"num_rows\": 51, \"num_elements\": 51}', schema='{\"mlflow_colspec\": [{\"type\": \"string\", \"name\": \"text\", \"required\": true}]}', source='{\"uri\": \"text_data/text_train_2307.08621.csv\"}', source_type='local'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.inputs.dataset_inputs[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uri': 'text_data/text_train_2307.08621.csv'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_source = mlflow.data.get_source(dataset_info)\n",
    "dataset_source.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from the run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioned_df = pd.read_csv(dataset_source.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Retentive Network: A Successor to Transformer\\...\n",
      "1  ence, which improves decoding throughput, late...\n",
      "2  Figure 1: Retentive network (RetNet) achieves ...\n",
      "3  els [BMR+20], which was initially proposed\\nto...\n",
      "4  “impossible triangle” as shown in Figure 2.\\nT...\n"
     ]
    }
   ],
   "source": [
    "print(versioned_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
